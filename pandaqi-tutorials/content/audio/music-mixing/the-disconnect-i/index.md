---
title: "The Disconnect I"
weight: 2
type: "article"
---

I also wrote a great chapter about this topic for my recording course: [The Disconnect](../../recording/the-disconnect/). That one focuses more on details and examples related to _recording_. In this one, I shift the focus to what this concept means for _mixing_.

What is the (ominously named) **disconnect**?

It's why mixing seems so hard and intuitive. It's because ...

> How our _ears_ work, is not how _microphones_ and _speakers_ work. 

It's because our ears are used to the real world, to hearing sounds in a physical space. We can easily tell where the sound is, how far away, in what kind of room we are (based on the reverb). Even if it's very soft or it comes from behind.

When you record something, and turn it into digital files, it's like entering an entirely new world. You don't have the laws of physics anymore. Your computer or microphone is not as amazing as your ears. Instead, it's your _job_ as an audio engineer to make those "bad" recordings sound like the real world _as much as possible_. Using tools, tricks and techniques.

Let's go over the most important differences. And why every digital recording will sound worse than the live sound (barring lucky exceptions).

## Mono / Stereo

We have _two_ ears. This means we hear a sound two times, but slightly differently. It enters the other ear a millisecond later. The other ear points to the _other side_, so it picks up a slightly different range of sounds.

Our brain performs a magical process: it combines these two inputs into one that sounds _great_. If you listen to a live piano, you're actually hearing two different things (one input from each ear), but your brain is smart enough to combine them. Into _one_ sound that is _realistic_: it sounds 3D and you know where it comes from. 

{{% remark %}}
The same is true for headphones, of course. We don't consciously hear the _two different earbuds_. Instead, in our head, we hear _one_ song, which is roughly "centered" in our mind.
{{% /remark %}}

This is called **stereo**: two channels. We understand and hear sound in _stereo_.

Now let's say you place a single microphone to record a guitar. That's a _single_ source, which we call **mono**. How could it ever capture the sound we hear with our ears?

It can't.

Because it's just one channel.

Even the best microphone, placed in the best possible way, will not sound as good (and "3D") as our ears. Not by default. It's your job, as audio engineer, to _turn_ that recording into something our ears like more. 

More on this in the chapter about [Panning](../panning/). That's the tool every DAW provides for placing tracks more to the left ear or right ear.

## Proximity

Again, our ears are amazing, microphones struggle to replicate it.

Most microphones have the _proximity_ effect. The closer you get, the more it captures _low frequencies_ (instead of giving a balanced image). Similarly, if you get further away, it will capture more of the _high frequencies_.

As such, a recording might not sound entirely like the sound you want. It has captured the sound ... but twisted it in a way. 

Similarly, microphones only hear in a very short range _and_ distort sound heard from the side. If your source (like a singer) moves around, even a tiny bit, that can cause drastic changes in volume or frequency content. One moment the recording is loud, the next you can barely hear the singer.

Our ears would not struggle with this at all. But a microphone picks up all these small changes and modifies the final recording.

It's best to _prevent_ this during the recording stage, of course. Otherwise, it's your job to find these quirks and combat them.

More on this in the chapters about [Compression](../compression/) and [Equalizer](../equalizer/).

## Reverb

Audio _bounces around_. An audio source produces sound in all directions. Then, these waves travel in straight lines until they hit something and get deflected. (Like throwing a ball against a wall.)

Every deflection, they get softer and higher. But some of those reflections---quite many, in fact---will reach your ears again.

This is _reverb_. The sound _reverberates_ off the room or the objects around you.

This reverb helps us immensely. It allows us to place sounds in 3D. If the reverb comes back very quick, we're in a tiny room. If the reverb is massive, we're in a large reflective room with a large ceiling---like a church or cathedral.

When you record something, you not only get the original sound, you also get all that reverb!

On its own, this might sound great and realistic. 

{{% remark %}}
Although, because of the limitations of microphones, this can already sound terrible. The original sound clashes with the reverb and makes the recording worse.
{{% /remark %}}

But almost all projects have more than one recording. In fact, they have many recordings played simultaneously.

In real life, the reverb from these instruments would mix into something nice and realistic.

But with digital recordings? Made in different locations or at different times? They all have a different reverb, and they'll all but heads, creating an ugly washed-out song.

That's why you need recordings that are very "dry". They were recorded in a good way, or a good (acoustically treated) space, to reduce that reverb.

Dry recordings usually don't sound great on their own. Because our ears want that reverb! It sounds unrealistic and dull!

But because they are "dry", you can combine and edit recordings easily. And you can add effects _later_ to get that reverb back---in a good way.

More on that later in the chapter [Reverb & Delay](../reverb-and-delay/).

## Our ears adapt

Our ears are living organs. They are connected to our brain, which is, you know, _smart_. Through a lifetime of experience with sounds, they are able to adapt. 

Annoying sounds get filtered out. If somebody speaks unclearly, we focus more on them to hear them better. A constant or repeating sound will also be completely ignored after a while. When a sound is too _loud_, our ears go into self-protection and will reduce their sensitivity to all input.

{{% example %}}
I live right next to a busy road. A worn-down road that really should _not_ be used by heavy trucks---but of course they do. I am able to sleep through loud, jarring noise just fine. My ears filter it out for me. But sometimes I wake up early, for some reason, and hear the day start and the first trucks come in. Then I realize just how incredibly noisy it is. Once I focus on it, consciously, I can't filter it out and get blasted with truck noise.
{{% /example %}}

Microphones can't do this, of course not. Another reason for the disconnect.

But it also means that it's hard to stay _objective_ when listening to our own mixes. After hearing the same section for a while, you don't actually listen to it anymore. You know it by heart. Your ears have adjusted. So you simply _can't_ make well-informed mixing decisions anymore.

Your song might sound terrible. But you'll only hear that during the first five seconds! Beyond that, your ears have now adjusted and don't mind the terrible audio quality anymore. 

{{% remark %}}
This is obviously also a benefit, as potential listeners will do the same. Most likely, though, they turn off your song within those ugly first five seconds.
{{% /remark %}}

The only way to stay objective, is to ...

* Switch between different sections / instruments / songs regularly
* Alternate between doing your mixing and just listening to the radio or your playlist
* Take many breaks

"Grinding" is never great. But especially in music production, forcing yourself to sit down for hours and "get that track done" is completely counterproductive. After 30-60 minutes (at most) your ears will either think everything you do sounds like shit or sounds like god composed it himself---and both opinions are wrong.