---
title: "The 3D space"
weight: 4
type: "article"
---

To me, mixing is about two "holy trinities". Everything else is not that important. And if it is, you'll learn those details only from experience and mixing a lot.

## Controls

The first trinity is: **Controls**
* Timing
* Volume
* Panning

These controls are in every software, and quick to use, because this is what you do 99% of the time. You pick which sounds to play (and when they start/end). You pick their volume. And you pick how to pan them: how much goes to the left ear or the right ear.

## Effects

The second trinity is: **Effects**

* Compression
* Equalizer (EQ)
* Reverb & Delay

These are also in every software, bundled by default, readily accessible and usable. Because this is all you need 99% of the time. Compression massages your volume. Equalizer deals with your frequencies. Reverb & Delay handle your space and timing.

## The 3D space

Combined, they give you the **3D space of mixing**! With just these controls and tools, you can _place_ sounds in a 3D space. Which will make your song as pretty and professional as possible. The listener will feel like they're on stage with the band, in a beautiful venue with great acoustics.

Usually, the listener is placed at the seat of the drummer or the vocalist. From their perspective, place all the other instruments of the band like theyâ€™d be positioned on a stage.

To me, this is what mixing boils down to. This is all you really need to remember---or "realize" at some point in your journey.

It's your job to ...

* Take a bunch of recordings. (Which sound disconnected and unnatural to our advanced ears. Remember last chapter!)
* And _combine_ them into a single space, as if they're giving a single performance together
* (And on a stage, no two performers will stand on each other's toes! So carve out a space for every part.)
* While sidestepping common issues with audio (recording, digitizing, reproduction, etcetera)

And just those two trinities---six concepts in total---is enough for 99% of that job.

The details will become more and more clear as you read this guide. But below is a summary to get you started.

### Height

**Height** is the most vague. Our ears are great at hearing if a sound comes from left or right, but _not_ if it's high or low. Still, there's some relationship here that's easy to see: high frequencies feel higher. Similarly, loud or bombastic sounds feel higher.

This might be because we _imagine_ sounds in our head. So high frequency = high in the air. It might also be because low frequencies can _travel_ through the floor or through objects. And loud sounds usually _don't_ come from below.

### Width

**Width** is more concrete and simple. It comes through panning and delay. Place one instrument in your _left_ ear, the other in your _right_, and you have width. Or play the same sound on both sides, but _delayed_ in one. Our head will imagine this as a single sound that traveled through that space. This adds width.

### Depth

**Depth** is most versatile and interesting. This is where 90% of your decision making will be. What parts to pull to the front and send to the back? And _how_ to do it, while keeping it audible or preventing clashes with other elements?

Some examples are,

* Louder = closer. (Things further away are obviously less loud, as all humans know.)
* Fuller sounds (with all frequencies) = closer. Such sounds have warm / low frequencies and high / crisp ones. (Things further away lose those details, becoming more of a washed-out thin sound.)
* Less reverb = closer. (So an easy way to push something back, is by applying a generous reverb plugin.)
* Spiky transients = closer. (A transient is the loudest part of a sound, when a note is struck or a hit starts. If you cut these off, or smooth them, sounds feel less immediate and further away.)

And in general, such depth can only come from _extremes_. If there's not enough _contrast_ between parts, they'll never feel far apart to the listener. So to apply depth, you need to skillfully use the _full_ spectrum of volumes and frequencies that an ear can hear.

## Arrangement

In my course on [Songwriting](../../songwriting/) I talked a great deal about arrangement. It's not just enough to have some chords and some lyrics, a professional song needs a full _arrangement_: every layer, every instrument, the exact part they'll be playing, the exact tempo (and changes) throughout the song, etcetera.

With mixing, arrangement isn't just "important". Mixing _is_ arrangement. It's exactly the same. It's your job to take existing recordings and ideas, then _arrange_ them into that final mix.

This means you'll spend little time actually _writing_ the song or _recording_ new bits. Instead, mixing is all about finding the right layers that will complement each other. If you already have a few recordings that mostly have _low_ frequencies, purposely choose other recordings that have _high_ frequencies for balance. If the vocal is quite repetitive, imagine tricks or sounds you could add that make the arrangement more varied.

Throughout the course, I'll give numerous examples of this. But I can tell you this: almost no mix is what you _think_ you hear. When you think it's one guitar, it might actually be 2 or 4. When you think it's one vocal take, it might be ten takes spliced together. When you hear a chorus and think "wow that sounds so POWERFUL", you subconsciously hear the ten tricks the audio engineer used to add this power.

But this also has a downside. If the original recordings are bad---if the original song is bad---no amount of mixing can save it. You arrange with what you have. Mixing is not about creating magic from nothing!

## To infinity and beyond!

The rest of this course discusses each concept (volume, panning, compression, ...) in-depth using its own article. Now you already know _why_ and _how_ they are important, which should aid the learning process.

I don't recommend reading this whole guide at once. You'll forget most of it, or only understand something in theory. 

Instead, after reading a new chapter (about the next control or effect), take a break and _apply_ it. See if you can hear for yourself (whatever tip or principle I gave). See if you can break the rules and what that sounds like.

{{% remark %}}
The tool I use for interactive examples was written, by me, using standard website code. All browsers have supported _some_ functionality for manipulating audio for quite some time. 

Funnily enough, when I went through the list, my holy trinities were there. The list of supported audio nodes was **exactly** this list I wrote above :) Fortunately, the people who write audio web standards agree with me!
{{% /remark %}}